{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis with CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN4nwyiOue06w8sSpnKa7I0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harenlin/Sentiment-Analysis-With-Tensorflow/blob/main/Sentiment_Analysis_with_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4VKUZL0I1QA"
      },
      "source": [
        "# Sentiment Analysis with CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2LU1U9OI47C"
      },
      "source": [
        "import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkXsVIOiIxp5",
        "outputId": "8dc39b34-d228-4d8c-b92f-e65e16e49504"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Reshape, Embedding, Activation\n",
        "from keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPool2D, Input, concatenate\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMPM46xwI68J"
      },
      "source": [
        "load in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzUFn2_8I7CS",
        "outputId": "2df286a8-4ec0-4a17-daac-388e3f2cf149"
      },
      "source": [
        "# hyper-parameters\n",
        "vocab_size = 3000\n",
        "max_seq_len = 300\n",
        "embedding_dim = 100\n",
        "\n",
        "# load-in dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "# x_train = array of indices, you can see whats in it\n",
        "print(x_train[1]) # [1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 2, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 2, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 2, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
        "print(x_train.shape) # (25000,)\n",
        "print(y_train.shape) # (25000,)\n",
        "print(x_test.shape)  # (25000,)\n",
        "print(y_test.shape)  # (25000,)\n",
        "\n",
        "# pad the sequence \n",
        "x_train = pad_sequences(x_train, maxlen = max_seq_len)\n",
        "x_test = pad_sequences(x_test, maxlen = max_seq_len)\n",
        "print(x_train.shape) # (25000, 300)\n",
        "print(x_test.shape)  # (25000, 300)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 2, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 2, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 2, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n",
            "(25000, 300)\n",
            "(25000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq8gVkqHJDK9"
      },
      "source": [
        "Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCuGVY66JBYa"
      },
      "source": [
        "# different size of kernal\n",
        "filter_sizes = [3,4,5]\n",
        "\n",
        "def convolutions(vocabulary_size=vocab_size, embedding_dimension=embedding_dim, max_seq_len=max_seq_len, filter_sizes=filter_sizes):\n",
        "    inputs = Input(shape = (max_seq_len, embedding_dimension, 1))\n",
        "    cnns = []\n",
        "    for size in filter_sizes:\n",
        "        cnn = Conv2D(filters=64, kernel_size=(size, embedding_dimension), strides=1, padding='valid', activation='relu')(inputs)\n",
        "        pooling = MaxPool2D(pool_size=(max_seq_len-size+1, 1), padding='valid')(cnn)\n",
        "        cnns.append(pooling)\n",
        "    cnns_outputs = concatenate(cnns)\n",
        "    model = Model(inputs=inputs, outputs=cnns_outputs)\n",
        "    return model\n",
        "\n",
        "def cnn_nlp_model(vocabulary_size=vocab_size, embedding_dimension=embedding_dim, max_seq_len=max_seq_len, filter_sizes=filter_sizes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocabulary_size, output_dim=embedding_dimension, input_length=max_seq_len),\n",
        "        Reshape(target_shape=(max_seq_len, embedding_dimension, 1)), # 2D -> 3D\n",
        "        convolutions(vocab_size, embedding_dim, max_seq_len, filter_sizes),\n",
        "        Flatten(),\n",
        "        Dense(10, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI-fhbldJJ8i"
      },
      "source": [
        "Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAzJXBo3JE_b",
        "outputId": "64b9360f-6e88-44ba-95eb-328cb25b666e"
      },
      "source": [
        "model = cnn_nlp_model(vocab_size, embedding_dim, max_seq_len, filter_sizes)\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 100)          300000    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 300, 100, 1)       0         \n",
            "_________________________________________________________________\n",
            "model (Functional)           (None, 1, 1, 192)         76992     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                1930      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 378,933\n",
            "Trainable params: 378,933\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "313/313 [==============================] - 64s 60ms/step - loss: 0.5831 - accuracy: 0.6560 - val_loss: 0.3092 - val_accuracy: 0.8736\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.2841 - accuracy: 0.8913 - val_loss: 0.2888 - val_accuracy: 0.8840\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 18s 59ms/step - loss: 0.2076 - accuracy: 0.9318 - val_loss: 0.2855 - val_accuracy: 0.8842\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 19s 59ms/step - loss: 0.1377 - accuracy: 0.9582 - val_loss: 0.3145 - val_accuracy: 0.8816\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 19s 60ms/step - loss: 0.0821 - accuracy: 0.9794 - val_loss: 0.3594 - val_accuracy: 0.8826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMYm_CFIJRI7"
      },
      "source": [
        "Second way to define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejQC33FGJMTV",
        "outputId": "08ef7563-b1a2-4736-fc46-f7a467b3d364"
      },
      "source": [
        "# different size of kernal\n",
        "filter_sizes = [3,4,5]\n",
        "\n",
        "def convolutions(vocabulary_size=vocab_size, embedding_dimension=embedding_dim, max_seq_len=max_seq_len, filter_sizes=filter_sizes):\n",
        "    inputs = Input(shape = (max_seq_len, embedding_dimension, 1))\n",
        "    cnns = []\n",
        "    for size in filter_sizes:\n",
        "        cnn = Conv2D(filters=64, kernel_size=(size, embedding_dimension), strides=1, padding='valid', activation='relu')(inputs)\n",
        "        pooling = MaxPool2D(pool_size=(max_seq_len-size+1, 1), padding='valid')(cnn)\n",
        "        cnns.append(pooling)\n",
        "    cnns_outputs = concatenate(cnns)\n",
        "    model = Model(inputs=inputs, outputs=cnns_outputs)\n",
        "    return model\n",
        "\n",
        "def cnn_nlp_model(vocabulary_size=vocab_size, embedding_dimension=embedding_dim, max_seq_len=max_seq_len, filter_sizes=filter_sizes):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_dimension, input_length=max_seq_len))\n",
        "    model.add(Reshape(target_shape=(max_seq_len, embedding_dimension, 1))) # 2D -> 3D\n",
        "    model.add(convolutions(vocab_size, embedding_dim, max_seq_len, filter_sizes))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "model = cnn_nlp_model(vocab_size, embedding_dim, max_seq_len, filter_sizes)\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 300, 100)          300000    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 300, 100, 1)       0         \n",
            "_________________________________________________________________\n",
            "model_1 (Functional)         (None, 1, 1, 192)         76992     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1930      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 378,933\n",
            "Trainable params: 378,933\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "313/313 [==============================] - 20s 61ms/step - loss: 0.5801 - accuracy: 0.6654 - val_loss: 0.3097 - val_accuracy: 0.8654\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 19s 60ms/step - loss: 0.2783 - accuracy: 0.8916 - val_loss: 0.2809 - val_accuracy: 0.8804\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 19s 60ms/step - loss: 0.1941 - accuracy: 0.9295 - val_loss: 0.3174 - val_accuracy: 0.8772\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 19s 60ms/step - loss: 0.1258 - accuracy: 0.9596 - val_loss: 0.3219 - val_accuracy: 0.8806\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 19s 61ms/step - loss: 0.0802 - accuracy: 0.9779 - val_loss: 0.3790 - val_accuracy: 0.8770\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hAdlTa4JVmP"
      },
      "source": [
        "Third way to define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TtB76FLJSt_",
        "outputId": "ab5e56df-6582-4bde-e33e-75fc95945b5c"
      },
      "source": [
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, vocabulary_size=vocab_size, embedding_dimension=embedding_dim, max_seq_len=max_seq_len, filter_sizes=filter_sizes):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dimension, input_length=max_seq_len)\n",
        "        self.reshape = Reshape(target_shape=(max_seq_len, embedding_dimension, 1)) # 2D -> 3D\n",
        "        self.cnn1 = Conv2D(filters=64, kernel_size=(filter_sizes[0], embedding_dimension), strides=1, padding='valid', activation='relu')\n",
        "        self.pool1 = MaxPool2D(pool_size=(max_seq_len-filter_sizes[0]+1, 1), padding='valid')\n",
        "        self.cnn2 = Conv2D(filters=64, kernel_size=(filter_sizes[1], embedding_dimension), strides=1, padding='valid', activation='relu')\n",
        "        self.pool2 = MaxPool2D(pool_size=(max_seq_len-filter_sizes[1]+1, 1), padding='valid')\n",
        "        self.cnn3 = Conv2D(filters=64, kernel_size=(filter_sizes[2], embedding_dimension), strides=1, padding='valid', activation='relu')\n",
        "        self.pool3 = MaxPool2D(pool_size=(max_seq_len-filter_sizes[2]+1, 1), padding='valid')\n",
        "        self.flatten = Flatten()\n",
        "        self.fc = Dense(10, activation='relu')\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.out_linear = Dense(1, activation='sigmoid')\n",
        "  \n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.reshape(x)\n",
        "        x1 = self.cnn1(x)\n",
        "        x1 = self.pool1(x1)\n",
        "        x2 = self.cnn1(x)\n",
        "        x2 = self.pool1(x2)\n",
        "        x3 = self.cnn1(x)\n",
        "        x3 = self.pool1(x3)\n",
        "        x = concatenate([x1,x2,x3], axis=-1)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        if training: x = self.dropout(x, training=training)\n",
        "        x = self.out_linear(x)\n",
        "        return x\n",
        "\n",
        "model = Model(vocabulary_size=vocab_size, embedding_dimension=embedding_dim, max_seq_len=max_seq_len, filter_sizes=filter_sizes)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 18s 47ms/step - loss: 0.4985 - accuracy: 0.7502 - val_loss: 0.3448 - val_accuracy: 0.8560\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 15s 46ms/step - loss: 0.3058 - accuracy: 0.8759 - val_loss: 0.3017 - val_accuracy: 0.8700\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 15s 47ms/step - loss: 0.2310 - accuracy: 0.9139 - val_loss: 0.3032 - val_accuracy: 0.8764\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 15s 47ms/step - loss: 0.1742 - accuracy: 0.9402 - val_loss: 0.3074 - val_accuracy: 0.8792\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 15s 48ms/step - loss: 0.1274 - accuracy: 0.9586 - val_loss: 0.3467 - val_accuracy: 0.8770\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}